{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeOamPhyoIZff+yEO6vMbe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0011Ashwin/Google-Colab-Work/blob/main/student_classroom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "eiIQxQVJ8Y8z",
        "outputId": "50bd9107-33f5-422b-c97f-d1570108867b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Detection Finished! Total frames processed: 818\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e0eb82c7-88e6-4345-bec6-d47519f0ab76\", \"classroom_output.mp4\", 45476242)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ------------------------------------\n",
        "# ✅ Install packages if needed\n",
        "#!pip install -q ultralytics opencv-python\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import clear_output\n",
        "from google.colab import files\n",
        "\n",
        "# ------------------------------------\n",
        "# ✅ Upload the Input Video\n",
        "\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "    video_path = '/content/' + fn\n",
        "    print(f\"✅ Uploaded file: {video_path}\")\n",
        "\n",
        "# ------------------------------------\n",
        "# ✅ Open the Video\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    raise Exception(\"❌ Error opening input video. Please upload a correct video.\")\n",
        "\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "duration = total_frames / fps if fps > 0 else 0\n",
        "\n",
        "print(f\"🎥 Video loaded: {total_frames} frames, {fps:.2f} FPS, {duration:.2f} seconds.\")\n",
        "\n",
        "if total_frames < 10:\n",
        "    raise Exception(\"⚠️ Input video is too short! Upload a longer video (more than 2 seconds).\")\n",
        "\n",
        "# ------------------------------------\n",
        "# ✅ Setup Output Video\n",
        "\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "output_path = '/content/classroom_output.mp4'\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "# ------------------------------------\n",
        "# ✅ Load the YOLOv8 Pose Model\n",
        "\n",
        "model = YOLO('yolov8n-pose.pt')  # 'n' is fast. Use 's', 'm', 'l' if you want better accuracy\n",
        "\n",
        "# ------------------------------------\n",
        "# ✅ Stand/Sit Classification Function\n",
        "\n",
        "def classify_pose_stand_or_sit(keypoints):\n",
        "    \"\"\"\n",
        "    Classify if student is 'Standing' or 'Sitting' based on keypoints (hips and shoulders).\n",
        "    \"\"\"\n",
        "    if keypoints is None or len(keypoints) < 17:\n",
        "        return 'Unknown'\n",
        "\n",
        "    left_hip = keypoints[11][:2]\n",
        "    right_hip = keypoints[12][:2]\n",
        "    left_shoulder = keypoints[5][:2]\n",
        "    right_shoulder = keypoints[6][:2]\n",
        "\n",
        "    hip_mid = (left_hip + right_hip) / 2\n",
        "    shoulder_mid = (left_shoulder + right_shoulder) / 2\n",
        "\n",
        "    vertical_distance = hip_mid[1] - shoulder_mid[1]  # y-axis\n",
        "\n",
        "    # Heuristic rule\n",
        "    if vertical_distance > 100:\n",
        "        return 'Standing'\n",
        "    else:\n",
        "        return 'Sitting'\n",
        "\n",
        "# ------------------------------------\n",
        "# ✅ Process the Video\n",
        "\n",
        "frame_count = 0\n",
        "saved_frames = 0\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break  # End of video\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "    # Run pose detection\n",
        "    results = model.predict(frame, imgsz=640, conf=0.4, verbose=False)\n",
        "\n",
        "    annotated_frame = results[0].plot()\n",
        "\n",
        "    # Process detected people\n",
        "    for person in results[0].keypoints.xy:\n",
        "        keypoints = person.cpu().numpy()\n",
        "\n",
        "        # Add dummy confidence 1.0\n",
        "        keypoints_full = np.concatenate([keypoints, np.ones((17,1))], axis=1)\n",
        "\n",
        "        action = classify_pose_stand_or_sit(keypoints_full)\n",
        "\n",
        "        if keypoints.shape[0] > 0:\n",
        "            x, y = int(keypoints[0][0]), int(keypoints[0][1])\n",
        "            cv2.putText(annotated_frame, action, (x, y - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "    # Save annotated frame\n",
        "    out.write(annotated_frame)\n",
        "    saved_frames += 1\n",
        "\n",
        "    # Show frame every 10 frames\n",
        "    if frame_count % 10 == 0:\n",
        "        cv2_imshow(annotated_frame)\n",
        "        clear_output(wait=True)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "print(f\"✅ Detection Finished! Total frames processed: {saved_frames}\")\n",
        "\n",
        "# ------------------------------------\n",
        "# ✅ Download the output video\n",
        "files.download(output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wVruNcArAa11"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}